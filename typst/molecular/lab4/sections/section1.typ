= Теоретические замечания

== Макроскопические системы и два способа их описания

Под макроскопическими системами (макросистемами) понимается любое тело в твердом, жидком или газообразном состоянии (плазма, электронный газ, излучение в полости), 
состоящее из колоссального количества структурных элементов - атомов или молекул. Существуют два способа описания свойств макросистем - термодинамический и статистический. 
Соответствующие этим способам науки называются соответственно термодинамика и статистическая физика (или молекулярная физика).

Термодинамика явилась исторически первой наукой, на основе которой стало возможно систематическое изучение макросвойств и процессов для произвольных систем с числом частиц $N >> 1$. 
В частности, к макросвойствам относятся давление $p$, объем $V$ и температура $T$. 
Предмет изучения термодинамики и статистической физики один и тот же (физикохимические свойства материальных систем), но методы их исследования различны.

При термодинамическом подходе о макросвойствах системы судят на основе обобщений экспериментальных исследований взаимного превращения теплоты и работы. 
Путем аналитического обобщения опытов сформулированы три закона термодинамики, которые составляют ее фундамент. 
На основе этих законов проводятся многочисленные исследования макросвойств и процессов произвольных систем с числом частиц $N >> 1$. 
Такой метод исследования известен как феноменологический. Феноменологический метод, будучи весьма простым, позволяет получать всегда правильные результаты. 
В то же время ввиду того, что при феноменологическом подходе полностью игнорируется молекулярное строение вещества, выводы, полученные в этих рамках, 
не позволяют глубоко вскрыть природу изучаемых явлений. Оставаясь в рамках термодинамики, невозможно обосновать законы термодинамики (дифференциальную связь между макросвойствами). 
Это свидетельствует об ограниченных возможностях феноменологического подхода.

Статистический метод с самого начала исходит из связи между свойствами частиц макросистемы, опираясь на их модели. 
В самом деле, свойства макросистем (физические, химические) не могут зависеть ни от чего иного, кроме как от свойств самих частиц, из которых она состоит, и внешних условий, 
в которых находится эта система.

Итак, система состоит из многочисленных частиц, число которых $N >> 1$. 
Можно ли рассчитать свойства системы, используя чисто механический подход к решению задачи, а именно оценить свойства системы по данным о скоростях, 
координатах частиц, которые могут быть получены путем решения уравнений движения?

Уравнения движения для $N$ частиц имеют вид:
$ m_i frac(d^2  r_i, d t^2) = sum_(k=1, \  k eq.not i)^(N) f_k $
где $m_i$ и $r_i$ - масса и координата $i$-частицы; $sum_(k=1, \ k eq.not i)^(N) f_k$ - сила, действующая на $i$-частицу со стороны остальных частиц системы.

Решая эти уравнения механики, определим зависимость скорости $v_i$ от времени $t$. 
Но такой механический подход к описанию макросвойств системы с большим числом частиц является не только чрезвычайно трудоемким, но и принципиально непригодным, 
так как даже в случае невзаимодействующих частиц для решения уравнений движения необходимо знание начальных условий для каждой частицы, а это невозможно из-за огромного их числа. 
При столкновении частиц начальные условия будут все время меняться, что еще более усложнит задачу. 
Поэтому должны появиться качественно новые закономерности, не сводящиеся к классическим механическим. 
Эти новые закономерности называют статистическими, и они присущи любым макросистемам с числом частиц $N >> 1$.

В основе статистического метода лежат молекулярно-кинетические представления. 
Он основывается на свойствах частиц, из которых состоит макросистема, на особенностях их движения и взаимодействия между собой и с окружающими телами. 
На основе таких молекулярно-кинетических представлений становится возможной глубокая интерпретация физикохимических свойств и процессов в макроскопической системе. 
Статистические закономерности изучаются математической статистикой на основе теории вероятностей. 

== Некоторые сведения из теории вероятностей

Событиями или случаями в теории вероятностей называют всякие явления, относительно которых имеет смысл ставить вопрос, могут они происходить или нет. 
Опыт или совокупность условий, в результате которых появляется то или иное событие, в теории вероятностей называется испытанием.

Если при данных условиях событие обязательно произойдет, то оно называется достоверным. Если же оно произойти не может, то его называют невозможным. 
Например, если начертить треугольник, то событие, состоящее в том, что получится треугольник, у которого каждая сторона короче суммы двух других сторон, есть достоверное событие. 
Появление треугольника, у которого одна из сторон длиннее суммы двух других, есть также событие, хотя и невозможное в Евклидовом пространстве.

Событие называется случайным, если в результате испытания оно может как произойти, так и не произойти. Например, при игре в орлянку может выпасть либо орел, либо решка. Это случайные события.

Пусть имеется некоторая макроскопическая система, находящаяся в заданном состоянии. 
Предположим, что какая-то характерная для системы величина $x$ может иметь дискретные значения $x_1, x_2, dots, x_i, dots, x_s$.

Осуществим над системой очень большое число $N$ измерений величины $x$, приводя систему перед каждым измерением в одно и то же исходное состояние. 
Вместо того чтобы производить повторные измерения над одной и той же системой, можно взять $N$ одинаковых систем, находящихся в одном и том же состоянии, 
и осуществить однократное измерение величины $x$ у всех этих систем. Такой набор одинаковых систем, находящихся в одинаковом состоянии, называется статистическим ансамблем.

Допустим, что $N_1$ измерений дали результат $x_1$, $N_2$ измерений - результат $x_2$, ..., $N_i$ измерений - результат $x_i$ и т. д. ( $sum N_i = N$ - число систем в ансамбле). 
Величина $v_i = N_i / N$ именуется относительной частотой появления результата $x_i$, а предел этой величины, получающийся при стремлении $N$ к бесконечности,
$ P_i = lim_(N -> infinity) frac(N_i, N) = lim_(N -> infinity) v_i $<eq:eq2>
называется вероятностью появления результата $x_i$. Для упрощения формул будем писать выражение для вероятности в виде $N_i / N$, подразумевая, что производится предельный переход при $N -> infinity$.

Поскольку $sum N_i = N$, то $sum P_i = sum frac(N_i, N) = 1$, т. е. сумма вероятностей всех возможных результатов измерения равна единице.

Существует еще одна интерпретация вероятности, применяющаяся в физике, аналогичная (@eq:eq2). Поясним ее на простейшем примере. Пусть в закрытом сосуде имеется одна молекула. 
Сталкиваясь со стенками, молекула претерпевает беспорядочные отражения от них, следующие друг за другом. При этом она побывает в различных местах сосуда. 
Выделим мысленно в сосуде какой-либо элементарный неподвижный объем $d V$ и будем наблюдать за молекулой в течение длительного времени $T$. 
В объеме $d V$ молекула побывает неоднократно, обозначим через $d t$ полное время пребывания молекулы в объеме $d V$. 
Отношение $(d t) / T$ называется относительным временем пребывания молекулы в объеме $d V$. Предел этого отношения
$ d P = lim_(T -> infinity) frac(d t, T) $<eq:eq3>
и есть вероятность нахождения молекулы в объеме $d V$.

Если рассматривать большое количество молекул в сосуде ($N >> 1$), то выражение (@eq:eq3) можно трактовать как вероятность того, 
что макросистема в произвольный момент времени находится в одном из микросостояний, в котором частицы имеют координаты $x_i + d x_i, y_i + d y_i, z_i + d z_i$ 
и импульсы $p_(x_i) + d p_(x_i), p_(y_i) + d p_(y_i), p_(z_i) + d p_(z_i)$, при этом давление, температура и объем системы не меняются.

Очевидно, что чем большим выбран элементарный объем $d V$, тем больше будет вероятность обнаружить в нем молекулу: $d P tilde.op d V$.

Введем коэффициент пропорциональности $rho$ - функцию координат $x_i, y_i, z_i$ и импульсов $p_(x_i), p_(y_i), p_(z_i)$ частиц. Тогда
$ d P = rho(x_i, y_i, z_i, p_(x_i), p_(y_i), p_(z_i)) d V $

Функция $rho(x, p)$ имеет смысл плотности вероятности и называется функцией статистического распределения или просто функцией распределения.

Знание явного вида функции распределения $rho$ необходимо, когда случайные величины могут принимать непрерывный ряд значений. 
С помощью функции распределения можно рассчитать любые средние свойства макросистем, 
например, среднее значение скорости частиц, число молекул, величина скорости и энергия которых попадают в определенные интервалы.

Явный вид функции $rho$ зависит от моделей, на основе которых описывается поведение частиц системы.

Примерами функций распределения являются распределение газовых молекул по скоростям Максвелла и распределение частиц по энергии в поле силы тяжести Больцмана. 
Функция распределения подчиняется условию нормировки $integral_V rho d V = 1$, определяющему вероятность достоверного события.

Важными понятиями в теории вероятностей и ее приложениях являются понятия среднего значения и математического ожидания. Разъясним эти понятия на конкретном примере. 
Пусть произведено $N$ однотипных измерений одной и той же величины $a$ при неизменных условиях. 
Пусть в $n_1$ случаях измеренное значение величины $a$ оказалось равным $a_1$, в $n_2$ случаях - $a_2$, ..., в $n_m$ случаях - $a_m$ ( $n_1 + n_2 + dots + n_m = N$ ). 
Среднее значение измеряемой величины определяется выражением
$ overline(a) = frac(n_1 a_1 + n_2 a_2 + dots + n_m a_m, N) = v_1 a_1 + dots + v_m a_m $<eq:eq5>

Допустим для простоты, что никаких других результатов, кроме $a_1, a_2, dots, a_m$ при измерениях появиться не может, так что эти результаты являются единственно возможными и несовместимыми. 
Тогда если неограниченно увеличивать число измерений $N$, то частоты $v_1, v_2, dots, v_m$ 
перейдут в свои предельные значения $P_1, P_2, dots, P_m$ - вероятности появления при измерениях значений $a_1, a_2, dots, a_m$. Выражение (@eq:eq5) при этом будет иметь вид
$ M(a) = P_1 a_1 + P_2 a_2 + dots + P_m a_m $<eq:eq6>
где $M(a)$ называется математическим ожиданием величины $a$.

Истинное значение измеряемой величины $a$, как правило, определить невозможно, так как измерения, сколь бы точны они ни были, сопровождаются ошибками. 
Исключения имеют место только при счете конечного числа предметов. Например, абсолютно точно можно сосчитать количество мест в зрительном зале или количество монет в копилке. 
Систематические ошибки могут быть исключены путем тщательного изучения приборов и методов измерения. Но случайные ошибки всегда остаются. Влияние их уменьшают путем многократного повторения измерений. 
Идеальной целью, к которой следовало бы стремиться на этом пути, является нахождение математического ожидания измеряемой величины. 
Оно и представляло бы окончательный результат измерения, выдаваемый экспериментатором за истинное значение измеряемой величины. 
Но нахождение математического ожидания требовало бы бесконечного повторения измерений, а потому на практике вместо него приходится довольствоваться средним значением, 
полученным в результате как можно большего числа измерений.

== Нормальный закон распределения

Наиболее распространенным в природе законом распределения случайных величин является закон нормального распределения (закон Гаусса). 
Например, нормальному закону следует распределение случайных ошибок при измерении любой физической величины, отклонение пуль при стрельбе по мишени вдоль любого направления от её центра, 
распределение газовых молекул по скоростям вдоль какого-либо направления. Кроме этого, многие другие распределения в предельных случаях приобретают структуру нормального закона. 
Простота нормального закона обеспечивает ему широкое применение, в том числе там, где лучшее приближение к истинному распределению оказывается возможным, но чрезвычайно трудоемким. 
Распределение Гаусса имеет место в том случае, если случайная величина зависит от большого числа независимых факторов, 
могущих вносить с равной вероятностью положительные и отрицательные отклонения вне зависимости от природы этих случайных факторов. 
Если влияние каждого из них невелико по сравнению с влиянием всей суммы, то закон распределения называется близким к нормальному.

Нормальный закон распределения характеризуется плотностью вероятности вида
$ rho(x, sigma) = frac(1, sqrt(2 pi) sigma) e^(-frac(x^2, 2 sigma^2)) $
где $sigma$ - параметр распределения, называемый стандартным отклонением величины $x$.

Кривая, иллюстрирующая зависимость плотности вероятности от аргумента в случае нормального распределения, называется кривой Гаусса. Она имеет симметричный холмообразный вид. 
Кривые нормального распределения при различных значениях стандартного отклонения ($sigma_2 > sigma_1$) представлены на @img1. 
Заштрихованная область иллюстрирует вероятность случайной величины $x$ принять значение в интервале между $x_0$ и $x_0 + Delta x$.
#figure(
  image("../images/img1.png", width: 50%),
  caption: "Кривая Гаусса"
)<img1>

Элементарными средствами анализа можно выяснить, что вся кривая расположена по одну сторону от оси абсцисс и симметрична относительно центра распределения ($x = 0$); 
при этом ось абсцисс является асимптотой кривой, а кривая имеет максимум, равный $frac(1, sigma sqrt(2 pi))$ в центре распределения и две точки перегиба при $x = plus.minus sigma$. 
Из такого вида кривой нормального закона распределения следуют его основные свойства. Перечислим их.

1. Нормальный закон симметричен, то есть равновеликие положительные и отрицательные отклонения величины $x$ от нулевого значения равновероятны.
2. Нормальный закон предписывает определённую вероятность любому отклонению величины $x$ от нулевого значения. Заранее исключенных случаев не имеется, так как $rho(x, sigma) > 0$ для любого $|x| < infinity$.
3. Нормальный закон указывает на одно определённое наивероятнейшее значение $x = 0$. Можно показать, что это значение отклонения есть математическое ожидание величины $x$. Для непрерывно распределенной величины $x$ формула (@eq:eq6) для математического ожидания будет иметь вид 
$ overline(x) = integral_(-infinity)^(infinity) x rho(x, sigma) d x $<eq:eq8>

Для случая нормального закона вследствие симметрии $rho(x, sigma)$ и нечетности сомножителя в подынтегральной функции получим математическое ожидание
$ overline(x) = integral_(-infinity)^(infinity) x rho(x, sigma) d x = integral_(0)^(infinity) x rho(x, sigma) d x + integral_(-infinity)^(0) x rho(x, sigma) d x = 0 $

Величина разброса возможных значений $x$ относительно их среднего значения, равная, в свою очередь, $x - overline(x) = x$ (т. е. самой себе), может быть охарактеризована с помощью параметров, 
называемых дисперсией и стандартным отклонением. Для уяснения необходимости введения этих параметров отметим, что среднее значение $overline(x)$ самой величины $x$ не может являться таким параметром, 
так как в среднем одинаково часто принимает как положительные, так и отрицательные значения. 
Поэтому, согласно свойству 3 нормального закона распределения, среднее значение разброса (математическое ожидание) величины $x$ равно 0, т. е. числу, не характеризующему величину разброса. 
Однако квадрат величины разброса возможных значений $x$ относительно среднего значения $overline(x)$ не может быть отрицательным. Его среднее значение, найденное по аналогии с (@eq:eq8), равно
$ D(x) = integral_(-infinity)^(infinity) (x - overline(x))^2 rho(x, sigma) d x = frac(1, sigma sqrt(2 pi)) integral_(-infinity)^(infinity) x^2 e^(-frac(x^2, 2 sigma^2)) d x $
и называется дисперсией величины $x$. Вычисление последнего интеграла дает величину $sigma^3 sqrt(2 pi)$. 
Таким образом, дисперсия величины $x$, распределенной по нормальному закону, выражается формулой
$ D(x) = sigma^2 $

Символическое обозначение $D(x)$ в данном случае означает не функциональную зависимость дисперсии от величины $x$, а её принадлежность к этой случайной величине. 
По характеру её определения дисперсия не может быть отрицательной, при этом она равна нулю только в случае, если все возможные значения $x$ равны 0. 
Значение дисперсии тем больше, чем больше вероятность получить значения $x$, заметно отличающиеся от среднего. Дисперсия имеет размерность квадрата величины $x$.

Линейной мерой разброса возможных значений $x$ является квадратный корень из дисперсии. Для нормального закона $sqrt(D(x)) = sqrt(sigma^2) = sigma$ - стандартное отклонение величины $x$. 
Смысл этой величины заключается в том, что большая часть возможных значений $x$ заключена в интервале $[-sigma, sigma]$, принадлежащем оси $x$ (ось абсцисс на @img1).

Таким образом, дисперсия и стандартное отклонение являются удобной мерой разброса случайной величины относительно ее среднего значения.

Структура формулы, выражающей плотность вероятности, удовлетворяет условию нормировки, то есть
$ integral_(-infinity)^(infinity) rho(x, sigma) d x = 1 $

Смысл нормировки заключается в том, что вероятность аргумента функции распределения принять одно из значений в интервале $(-infinity < x < infinity)$ равна единице. 
Такое событие в теории вероятности называется достоверным, то есть происходит обязательно.

